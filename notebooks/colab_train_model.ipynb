{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# ðŸ… LEMMA IMO Substitution Model - Google Colab\n",
                "\n",
                "**GPU Setup**: Runtime â†’ Change runtime type â†’ T4 GPU\n",
                "\n",
                "Training time: ~5 minutes"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Install and disable wandb\n",
                "import os\n",
                "os.environ['WANDB_DISABLED'] = 'true'\n",
                "\n",
                "!pip install transformers datasets scikit-learn onnx onnxruntime -q"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import torch\n",
                "print(f\"GPU: {torch.cuda.is_available()} - {torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'None'}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Substitution vocabulary\n",
                "VOCAB = [\n",
                "    \"x = 0\", \"y = 0\", \"x = y\", \"x = 1\", \"y = 1\",\n",
                "    \"a = b = c = 1\", \"abc = 1 constraint\", \"Apply AM-GM\",\n",
                "    \"Apply Cauchy-Schwarz\", \"Assume f is linear\", \"Assume f is injective\",\n",
                "    \"Assume f is monotonic\", \"Check small cases\", \"Use modular arithmetic\",\n",
                "    \"Homogenize\", \"WLOG assume ordering\", \"Substitute c = 1/(ab)\",\n",
                "    \"y = f(x)\", \"x = -y\", \"Consider p = 2 separately\",\n",
                "]\n",
                "\n",
                "# Training data\n",
                "DATA = [\n",
                "    {\"text\": \"Find all functions f: R â†’ R such that f(x + f(y)) = f(x) + y.\", \"subs\": [\"x = 0\", \"y = 0\", \"x = y\"]},\n",
                "    {\"text\": \"Find all functions f: Z â†’ Z such that f(2a) + 2f(b) = f(f(a + b)).\", \"subs\": [\"x = 0\", \"y = 0\"]},\n",
                "    {\"text\": \"Find all functions f: R â†’ R such that f(f(x)f(y)) + f(x + y) = f(xy).\", \"subs\": [\"x = 0\", \"y = 0\", \"x = 1\"]},\n",
                "    {\"text\": \"Prove for positive a,b,c with abc=1: (a-1+1/b)(b-1+1/c)(c-1+1/a) â‰¤ 1.\", \"subs\": [\"abc = 1 constraint\", \"Apply AM-GM\", \"a = b = c = 1\"]},\n",
                "    {\"text\": \"Prove for positive a,b,c: a/âˆš(aÂ²+8bc) + b/âˆš(bÂ²+8ca) + c/âˆš(cÂ²+8ab) â‰¥ 1.\", \"subs\": [\"Apply Cauchy-Schwarz\", \"a = b = c = 1\"]},\n",
                "    {\"text\": \"Find all positive integers n such that n divides 2^n + 1.\", \"subs\": [\"Check small cases\", \"Use modular arithmetic\"]},\n",
                "    {\"text\": \"Find all functions f: N â†’ N such that f(m + f(n)) = f(f(m)) + f(n).\", \"subs\": [\"x = 0\", \"y = f(x)\", \"Assume f is injective\"]},\n",
                "    {\"text\": \"Let a,b,c be positive with a+b+c=3. Prove aÂ² + bÂ² + cÂ² â‰¥ 3.\", \"subs\": [\"Apply Cauchy-Schwarz\", \"a = b = c = 1\"]},\n",
                "    {\"text\": \"Find functions f: Q â†’ Q with f(x + f(y)) = f(x) + y.\", \"subs\": [\"x = 0\", \"y = 0\", \"Assume f is linear\"]},\n",
                "    {\"text\": \"Find functions f: R â†’ R with f(xÂ²) - f(yÂ²) = (f(x)+y)(x-f(y)).\", \"subs\": [\"x = 0\", \"y = 0\", \"x = -y\"]},\n",
                "]\n",
                "\n",
                "print(f\"Vocab: {len(VOCAB)}, Data: {len(DATA)}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from transformers import DistilBertTokenizer, DistilBertForSequenceClassification, Trainer, TrainingArguments\n",
                "from sklearn.preprocessing import MultiLabelBinarizer\n",
                "from torch.utils.data import Dataset\n",
                "import numpy as np\n",
                "\n",
                "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
                "mlb = MultiLabelBinarizer(classes=VOCAB)\n",
                "mlb.fit([VOCAB])\n",
                "\n",
                "class SubsDataset(Dataset):\n",
                "    def __init__(self, data):\n",
                "        self.data = data\n",
                "    def __len__(self): return len(self.data)\n",
                "    def __getitem__(self, i):\n",
                "        enc = tokenizer(self.data[i]['text'], truncation=True, max_length=256, padding='max_length', return_tensors='pt')\n",
                "        lab = mlb.transform([self.data[i]['subs']])\n",
                "        return {'input_ids': enc['input_ids'].squeeze(), 'attention_mask': enc['attention_mask'].squeeze(), 'labels': torch.tensor(lab.squeeze(), dtype=torch.float)}\n",
                "\n",
                "train_ds = SubsDataset(DATA[:8])\n",
                "val_ds = SubsDataset(DATA[8:])\n",
                "print(f\"Train: {len(train_ds)}, Val: {len(val_ds)}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "model = DistilBertForSequenceClassification.from_pretrained(\n",
                "    'distilbert-base-uncased', num_labels=len(VOCAB), problem_type='multi_label_classification'\n",
                ").cuda()\n",
                "\n",
                "args = TrainingArguments(\n",
                "    output_dir='./out', num_train_epochs=30, per_device_train_batch_size=4,\n",
                "    eval_strategy='epoch', save_strategy='epoch', load_best_model_at_end=True,\n",
                "    metric_for_best_model='accuracy', fp16=True, report_to='none',\n",
                "    logging_steps=5, warmup_steps=20,\n",
                ")\n",
                "\n",
                "def metrics(p):\n",
                "    preds = (torch.sigmoid(torch.tensor(p.predictions)) > 0.5).numpy()\n",
                "    return {'accuracy': (preds == p.label_ids).mean()}\n",
                "\n",
                "trainer = Trainer(model=model, args=args, train_dataset=train_ds, eval_dataset=val_ds, compute_metrics=metrics)\n",
                "print(\"Ready to train!\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# TRAIN\n",
                "trainer.train()\n",
                "print(\"âœ… Training complete!\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Test prediction\n",
                "def predict(text, k=3):\n",
                "    model.eval()\n",
                "    inp = tokenizer(text, return_tensors='pt', max_length=256, truncation=True, padding='max_length')\n",
                "    inp = {k: v.cuda() for k, v in inp.items()}\n",
                "    with torch.no_grad(): logits = model(**inp).logits\n",
                "    probs = torch.sigmoid(logits).squeeze().cpu().numpy()\n",
                "    top = probs.argsort()[-k:][::-1]\n",
                "    return [(VOCAB[i], f\"{probs[i]:.0%}\") for i in top]\n",
                "\n",
                "print(\"Test: Find all functions f with f(x+y) = f(x) + f(y)\")\n",
                "for sub, prob in predict(\"Find all functions f with f(x+y) = f(x) + f(y)\"):\n",
                "    print(f\"  {prob} {sub}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Export ONNX\n",
                "import json\n",
                "from pathlib import Path\n",
                "\n",
                "out = Path('lemma_model')\n",
                "out.mkdir(exist_ok=True)\n",
                "\n",
                "model.cpu().eval()\n",
                "dummy = tokenizer(\"test\", return_tensors='pt', max_length=256, truncation=True, padding='max_length')\n",
                "torch.onnx.export(model, (dummy['input_ids'], dummy['attention_mask']), str(out/'model.onnx'),\n",
                "                  input_names=['input_ids', 'attention_mask'], output_names=['logits'], opset_version=14)\n",
                "\n",
                "with open(out/'vocab.json', 'w') as f: json.dump(VOCAB, f)\n",
                "tokenizer.save_pretrained(str(out))\n",
                "\n",
                "!zip -r lemma_model.zip lemma_model/\n",
                "print(\"âœ… Download lemma_model.zip from Files panel (left)\")"
            ]
        }
    ],
    "metadata": {\"kernelspec\": {\"display_name\": \"Python 3\", \"language\": \"python\", \"name\": \"python3\"}},"nbformat": 4,
        "nbformat_minor": 4
    }